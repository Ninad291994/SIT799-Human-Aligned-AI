{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPUjp1Gon49a"
   },
   "source": [
    "GitHub: https://github.com/MYUSER/MYPROJECT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REwNDNz8n49f"
   },
   "source": [
    "\n",
    "Welcome to your assignment this week! \n",
    "\n",
    "To better understand the adverse use of AI, in this assignment, we will look at a Natural Language Processing use case.\n",
    "\n",
    "\n",
    "Natural Language Pocessing (NLP) is a branch of Artificial Intelligence (AI) that helps computers to understand, to interpret and to manipulate natural (i.e. human) language.\n",
    "Imagine NLP-powered machines as black boxes that are capable of understanding and evaluating the context of the input documents (i.e. collection of words), outputting meaningful results that depend on the task the machine is designed for.\n",
    "\n",
    "\n",
    "![](imgs/1_3zMvUnPzYZF9CSHdj6hT5A.png)\n",
    "\n",
    "<caption><center> Documents are fed into magic NLP model capable to get, for instance, the sentiment of the original content</center></caption>\n",
    "\n",
    "\n",
    "In this notebook, you will implement a model that uses an LSTM to generate fake tweets and comments. You will also be able to try it to generate your own fake text. \n",
    "\n",
    "**You will learn to:**\n",
    "- Apply an LSTM to generate fake comments.\n",
    "- Generate your own fake text with deep learning.\n",
    "\n",
    "Please run the following cell to load all the packages required in this assignment. This may take a few minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4dMhfnDn49i"
   },
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jsz0I2Yn494"
   },
   "source": [
    "Run the following cell to load the packages you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21HQSzyRn497"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qYCYDFRn4-I"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFqQZGGln4-K"
   },
   "source": [
    "Let's define a tokenizer and read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGNJxCFQn4-N"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "data = open('covid19_fake.txt').read().replace(\".\", \" . \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"!\", \" ! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B90vwHi4n4-Z"
   },
   "source": [
    "Now, let's splits the data into tweets  where each line of the input file is a fake tweets.\n",
    "\n",
    "We also extract the vocabulary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NpMa3rqjn4-c"
   },
   "outputs": [],
   "source": [
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Qy083lcn4-m"
   },
   "source": [
    "You've loaded:\n",
    "- `corpus`: an array where each entry is a fake post.\n",
    "- `tokenizer`: which is the object that we will use to vectorize our dataset. This object also contains our word index.\n",
    "- `total_words`: is the total number of words in the vacabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "iNU2AVcQn4-n",
    "outputId": "f3d689e3-c9df-483b-a416-941cc58fb208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of fake tweets:  ['there is already a vaccine to treat covid19 . ', 'cleaning hands do not help to prevent covid19 . ']\n",
      "Size of the vocabulary =  1257\n",
      "Example of our word index =  [('.', 1), ('the', 2), ('covid19', 3), ('in', 4), ('to', 5), ('a', 6), ('of', 7), (',', 8), ('coronavirus', 9), ('and', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of fake tweets: \",corpus[:2])\n",
    "print(\"Size of the vocabulary = \", total_words)\n",
    "index = [(k, v) for k, v in tokenizer.word_index.items()]\n",
    "print(\"Example of our word index = \", index[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trchTs2Dn4-u"
   },
   "source": [
    "The next step aims to generate the training set of n_grams sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3OC9CHGn4-v"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZtI8pZLn4-1"
   },
   "source": [
    "You've create:\n",
    "- `input_sequences`: which is a list of n_grams sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Cs4PN-jln4-2",
    "outputId": "0530f2f5-3eb3-4bbd-8b6d-97953089b779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[2, 3, 12, 187, 34, 188]\n",
      " and it corresponds to:\n",
      "the covid19 is same as sars "
     ]
    }
   ],
   "source": [
    "sample = 20\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\" and it corresponds to:\")\n",
    "for i in input_sequences[sample]:\n",
    "    print(reverse_word_map[i], end=' ')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-8XcNFZn4-6"
   },
   "source": [
    "Next, we padd our training set to the max length in order to be able to make a batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kZh492ln4-7"
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbC8Fh7tn4--"
   },
   "source": [
    "Run the following to see the containt of the padded 'input_sequences' object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "K6NaKgDEn4-_",
    "outputId": "565f100e-5773-4b06-c5cf-eb2750871f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34 188]\n",
      " and it corresponds to:\n",
      "[ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ the covid19 is same as sars ]\n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\" and it corresponds to:\")\n",
    "print(\"[\", end=' ')\n",
    "for i in input_sequences[sample]:\n",
    "    if i in reverse_word_map:\n",
    "        print(reverse_word_map[i], end=' ')\n",
    "    else:\n",
    "        print(\"__\", end=' ')\n",
    "print(\"]\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VhsMw8Gn4_E"
   },
   "source": [
    "Given a sentence like **\"the covid19 is same as \"**, we want to design a model that can predict the next word -- in the case the word **\"sars\"**.\n",
    "\n",
    "Therefore, the next code prepares our input and output to our model consequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCoycSmEn4_F"
   },
   "outputs": [],
   "source": [
    "input_to_model, label = input_sequences[:,:-1],input_sequences[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "M8expSVVn4_I",
    "outputId": "1654aa45-6e75-473b-d4c3-37c89a09f6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34 188]\n",
      ", it corresponds to the following input to our model:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34]\n",
      " and the following output:  188\n"
     ]
    }
   ],
   "source": [
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\", it corresponds to the following input to our model:\")\n",
    "print(input_to_model[sample])\n",
    "print(\" and the following output: \", label[sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "am241lMfn4_L"
   },
   "source": [
    "Finally, we convert our label to categorical labels for being processed by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quOXCZidn4_L"
   },
   "outputs": [],
   "source": [
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubli_WVKn4_N"
   },
   "source": [
    "Here is the architecture of the model we will use:\n",
    "\n",
    "![](imgs/text_generation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9E8Vkdsn4_O"
   },
   "source": [
    " \n",
    "**Task 1**: Implement `deep_fake_comment_model()`. You will need to carry out 5 steps:\n",
    "\n",
    "1. Create a sequencial model using the `Sequential` class\n",
    "2. Add an embedding layer to the model using the `Embedding` class of size 128\n",
    "3. Add an LSTM layer to the model using the `LSTM` class of size 128\n",
    "4. Add a Dense layer to the model using the `Dense` class with a `softmax` activation\n",
    "5. Set a `categorical_crossentropy` loss function to the model and optimize `accuracy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wN8JoWDn4_P"
   },
   "outputs": [],
   "source": [
    "#TASK 1\n",
    "# deep_fake_comment_model\n",
    "size = 128\n",
    "def deep_fake_comment_model():\n",
    "    ### START CODE HERE ### \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(1257, size, input_length=60))\n",
    "    model.add(LSTM(size))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1257, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "#Print details of the model.\n",
    "model = deep_fake_comment_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WROAEd7fn4_R"
   },
   "source": [
    "Now, let's start our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oz9wmvGZn4_R",
    "outputId": "722d947a-f008-4e40-d629-894bd07d723d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "126/126 [==============================] - 1s 10ms/step - loss: 6.3508 - accuracy: 0.0695\n",
      "Epoch 2/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 5.8868 - accuracy: 0.0759\n",
      "Epoch 3/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.7512 - accuracy: 0.0811\n",
      "Epoch 4/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.6440 - accuracy: 0.1032\n",
      "Epoch 5/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.4931 - accuracy: 0.1241\n",
      "Epoch 6/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 5.3109 - accuracy: 0.1404\n",
      "Epoch 7/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 5.1229 - accuracy: 0.1578\n",
      "Epoch 8/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.9436 - accuracy: 0.1682\n",
      "Epoch 9/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.7736 - accuracy: 0.1821\n",
      "Epoch 10/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.6009 - accuracy: 0.1940\n",
      "Epoch 11/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.4358 - accuracy: 0.2057\n",
      "Epoch 12/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.2865 - accuracy: 0.2097\n",
      "Epoch 13/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 4.1190 - accuracy: 0.2243\n",
      "Epoch 14/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.9686 - accuracy: 0.2432\n",
      "Epoch 15/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.8318 - accuracy: 0.2538\n",
      "Epoch 16/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.6768 - accuracy: 0.2730\n",
      "Epoch 17/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.5424 - accuracy: 0.2958\n",
      "Epoch 18/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.4023 - accuracy: 0.3114\n",
      "Epoch 19/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.2651 - accuracy: 0.3313\n",
      "Epoch 20/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 3.1151 - accuracy: 0.3499\n",
      "Epoch 21/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.9814 - accuracy: 0.3747\n",
      "Epoch 22/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.8607 - accuracy: 0.3913\n",
      "Epoch 23/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.7332 - accuracy: 0.4208\n",
      "Epoch 24/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 2.5996 - accuracy: 0.4427\n",
      "Epoch 25/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 2.4733 - accuracy: 0.4784\n",
      "Epoch 26/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 2.3626 - accuracy: 0.5027\n",
      "Epoch 27/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 2.2453 - accuracy: 0.5186\n",
      "Epoch 28/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 2.1253 - accuracy: 0.5556\n",
      "Epoch 29/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 2.0232 - accuracy: 0.5821\n",
      "Epoch 30/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.9060 - accuracy: 0.6127\n",
      "Epoch 31/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.8227 - accuracy: 0.6303\n",
      "Epoch 32/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.7287 - accuracy: 0.6531\n",
      "Epoch 33/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.6393 - accuracy: 0.6635\n",
      "Epoch 34/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.5468 - accuracy: 0.7030\n",
      "Epoch 35/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.4674 - accuracy: 0.7065\n",
      "Epoch 36/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.3873 - accuracy: 0.7328\n",
      "Epoch 37/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.3313 - accuracy: 0.7434\n",
      "Epoch 38/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.2401 - accuracy: 0.7638\n",
      "Epoch 39/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.1855 - accuracy: 0.7737\n",
      "Epoch 40/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.1217 - accuracy: 0.7831\n",
      "Epoch 41/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.0793 - accuracy: 0.7950\n",
      "Epoch 42/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 1.0340 - accuracy: 0.8040\n",
      "Epoch 43/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.9812 - accuracy: 0.8134\n",
      "Epoch 44/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.9274 - accuracy: 0.8208\n",
      "Epoch 45/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.8923 - accuracy: 0.8337\n",
      "Epoch 46/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.8528 - accuracy: 0.8380\n",
      "Epoch 47/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.8166 - accuracy: 0.8474\n",
      "Epoch 48/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.7811 - accuracy: 0.8479\n",
      "Epoch 49/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.7414 - accuracy: 0.8563\n",
      "Epoch 50/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.7058 - accuracy: 0.8658\n",
      "Epoch 51/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.6923 - accuracy: 0.8655\n",
      "Epoch 52/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.6691 - accuracy: 0.8705\n",
      "Epoch 53/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.6239 - accuracy: 0.8811\n",
      "Epoch 54/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.6034 - accuracy: 0.8859\n",
      "Epoch 55/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.5842 - accuracy: 0.8849\n",
      "Epoch 56/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.5558 - accuracy: 0.8918\n",
      "Epoch 57/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.5335 - accuracy: 0.8995\n",
      "Epoch 58/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.5149 - accuracy: 0.9017\n",
      "Epoch 59/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.5042 - accuracy: 0.9022\n",
      "Epoch 60/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4795 - accuracy: 0.9022\n",
      "Epoch 61/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4706 - accuracy: 0.9022\n",
      "Epoch 62/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4502 - accuracy: 0.9079\n",
      "Epoch 63/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4395 - accuracy: 0.9112\n",
      "Epoch 64/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4236 - accuracy: 0.9102\n",
      "Epoch 65/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.4063 - accuracy: 0.9146\n",
      "Epoch 66/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3944 - accuracy: 0.9159\n",
      "Epoch 67/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3877 - accuracy: 0.9189\n",
      "Epoch 68/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3782 - accuracy: 0.9208\n",
      "Epoch 69/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3672 - accuracy: 0.9248\n",
      "Epoch 70/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3527 - accuracy: 0.9273\n",
      "Epoch 71/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.3425 - accuracy: 0.9268\n",
      "Epoch 72/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3302 - accuracy: 0.9280\n",
      "Epoch 73/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3261 - accuracy: 0.9315\n",
      "Epoch 74/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3110 - accuracy: 0.9305\n",
      "Epoch 75/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.3146 - accuracy: 0.9298\n",
      "Epoch 76/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.3120 - accuracy: 0.9290\n",
      "Epoch 77/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2982 - accuracy: 0.9300\n",
      "Epoch 78/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2878 - accuracy: 0.9337\n",
      "Epoch 79/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2879 - accuracy: 0.9308\n",
      "Epoch 80/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2810 - accuracy: 0.9377\n",
      "Epoch 81/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2832 - accuracy: 0.9355\n",
      "Epoch 82/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2676 - accuracy: 0.9362\n",
      "Epoch 83/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2660 - accuracy: 0.9390\n",
      "Epoch 84/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2622 - accuracy: 0.9347\n",
      "Epoch 85/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2623 - accuracy: 0.9340\n",
      "Epoch 86/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2547 - accuracy: 0.9372\n",
      "Epoch 87/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2458 - accuracy: 0.9387\n",
      "Epoch 88/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2467 - accuracy: 0.9402\n",
      "Epoch 89/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2435 - accuracy: 0.9370\n",
      "Epoch 90/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2423 - accuracy: 0.9350\n",
      "Epoch 91/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2311 - accuracy: 0.9409\n",
      "Epoch 92/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2340 - accuracy: 0.9432\n",
      "Epoch 93/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2266 - accuracy: 0.9434\n",
      "Epoch 94/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2289 - accuracy: 0.9404\n",
      "Epoch 95/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2249 - accuracy: 0.9424\n",
      "Epoch 96/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2150 - accuracy: 0.9414\n",
      "Epoch 97/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2154 - accuracy: 0.9442\n",
      "Epoch 98/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2183 - accuracy: 0.9395\n",
      "Epoch 99/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2108 - accuracy: 0.9427\n",
      "Epoch 100/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2073 - accuracy: 0.9437\n",
      "Epoch 101/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.2087 - accuracy: 0.9457\n",
      "Epoch 102/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2056 - accuracy: 0.9427\n",
      "Epoch 103/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9511\n",
      "Epoch 104/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.2021 - accuracy: 0.9447\n",
      "Epoch 105/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1966 - accuracy: 0.9469\n",
      "Epoch 106/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1952 - accuracy: 0.9449\n",
      "Epoch 107/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1964 - accuracy: 0.9439\n",
      "Epoch 108/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9462\n",
      "Epoch 109/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1939 - accuracy: 0.9459\n",
      "Epoch 110/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1960 - accuracy: 0.9427\n",
      "Epoch 111/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9457\n",
      "Epoch 112/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1864 - accuracy: 0.9481\n",
      "Epoch 113/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1836 - accuracy: 0.9481\n",
      "Epoch 114/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1826 - accuracy: 0.9464\n",
      "Epoch 115/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1834 - accuracy: 0.9457\n",
      "Epoch 116/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1873 - accuracy: 0.9432\n",
      "Epoch 117/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1872 - accuracy: 0.9459\n",
      "Epoch 118/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1801 - accuracy: 0.9452\n",
      "Epoch 119/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1783 - accuracy: 0.9467\n",
      "Epoch 120/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1748 - accuracy: 0.9479\n",
      "Epoch 121/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1761 - accuracy: 0.9467\n",
      "Epoch 122/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1766 - accuracy: 0.9481\n",
      "Epoch 123/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1811 - accuracy: 0.9452\n",
      "Epoch 124/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1688 - accuracy: 0.9479\n",
      "Epoch 125/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1726 - accuracy: 0.9471\n",
      "Epoch 126/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1743 - accuracy: 0.9462\n",
      "Epoch 127/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1729 - accuracy: 0.9481\n",
      "Epoch 128/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1717 - accuracy: 0.9462\n",
      "Epoch 129/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1688 - accuracy: 0.9462\n",
      "Epoch 130/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1684 - accuracy: 0.9469\n",
      "Epoch 131/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1666 - accuracy: 0.9462\n",
      "Epoch 132/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1683 - accuracy: 0.9499\n",
      "Epoch 133/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1637 - accuracy: 0.9481\n",
      "Epoch 134/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1621 - accuracy: 0.9474\n",
      "Epoch 135/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1632 - accuracy: 0.9501\n",
      "Epoch 136/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1653 - accuracy: 0.9474\n",
      "Epoch 137/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1577 - accuracy: 0.9519\n",
      "Epoch 138/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1666 - accuracy: 0.9474\n",
      "Epoch 139/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1673 - accuracy: 0.9444\n",
      "Epoch 140/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1652 - accuracy: 0.9481\n",
      "Epoch 141/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1606 - accuracy: 0.9499\n",
      "Epoch 142/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1641 - accuracy: 0.9484\n",
      "Epoch 143/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1587 - accuracy: 0.9509\n",
      "Epoch 144/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1627 - accuracy: 0.9471\n",
      "Epoch 145/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1627 - accuracy: 0.9476\n",
      "Epoch 146/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1580 - accuracy: 0.9479\n",
      "Epoch 147/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1604 - accuracy: 0.9481\n",
      "Epoch 148/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1569 - accuracy: 0.9494\n",
      "Epoch 149/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1608 - accuracy: 0.9484\n",
      "Epoch 150/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1629 - accuracy: 0.9474\n",
      "Epoch 151/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1577 - accuracy: 0.9476\n",
      "Epoch 152/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1552 - accuracy: 0.9484\n",
      "Epoch 153/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1577 - accuracy: 0.9484\n",
      "Epoch 154/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1557 - accuracy: 0.9476\n",
      "Epoch 155/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1630 - accuracy: 0.9439\n",
      "Epoch 156/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1595 - accuracy: 0.9476\n",
      "Epoch 157/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1562 - accuracy: 0.9464\n",
      "Epoch 158/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1542 - accuracy: 0.9476\n",
      "Epoch 159/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1507 - accuracy: 0.9496\n",
      "Epoch 160/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1468 - accuracy: 0.9514\n",
      "Epoch 161/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1536 - accuracy: 0.9494\n",
      "Epoch 162/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1491 - accuracy: 0.9481\n",
      "Epoch 163/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1575 - accuracy: 0.9471\n",
      "Epoch 164/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1492 - accuracy: 0.9504\n",
      "Epoch 165/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1537 - accuracy: 0.9496\n",
      "Epoch 166/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1552 - accuracy: 0.9474\n",
      "Epoch 167/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1546 - accuracy: 0.9486\n",
      "Epoch 168/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1522 - accuracy: 0.9474\n",
      "Epoch 169/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1520 - accuracy: 0.9479\n",
      "Epoch 170/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1493 - accuracy: 0.9521\n",
      "Epoch 171/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1497 - accuracy: 0.9489\n",
      "Epoch 172/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1458 - accuracy: 0.9496\n",
      "Epoch 173/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1463 - accuracy: 0.9506\n",
      "Epoch 174/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1609 - accuracy: 0.9474\n",
      "Epoch 175/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1652 - accuracy: 0.9432\n",
      "Epoch 176/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1550 - accuracy: 0.9474\n",
      "Epoch 177/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1527 - accuracy: 0.9471\n",
      "Epoch 178/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1470 - accuracy: 0.9494\n",
      "Epoch 179/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1475 - accuracy: 0.9509\n",
      "Epoch 180/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1495 - accuracy: 0.9459\n",
      "Epoch 181/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1517 - accuracy: 0.9501\n",
      "Epoch 182/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1503 - accuracy: 0.9481\n",
      "Epoch 183/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1493 - accuracy: 0.9471\n",
      "Epoch 184/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1453 - accuracy: 0.9506\n",
      "Epoch 185/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1448 - accuracy: 0.9504\n",
      "Epoch 186/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1472 - accuracy: 0.9504\n",
      "Epoch 187/200\n",
      "126/126 [==============================] - 1s 9ms/step - loss: 0.1479 - accuracy: 0.9506\n",
      "Epoch 188/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1486 - accuracy: 0.9474\n",
      "Epoch 189/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1417 - accuracy: 0.9511\n",
      "Epoch 190/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1447 - accuracy: 0.9499\n",
      "Epoch 191/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1493 - accuracy: 0.9494\n",
      "Epoch 192/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1426 - accuracy: 0.9516\n",
      "Epoch 193/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1489 - accuracy: 0.9474\n",
      "Epoch 194/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1452 - accuracy: 0.9491\n",
      "Epoch 195/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1423 - accuracy: 0.9501\n",
      "Epoch 196/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1445 - accuracy: 0.9484\n",
      "Epoch 197/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1401 - accuracy: 0.9516\n",
      "Epoch 198/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1450 - accuracy: 0.9489\n",
      "Epoch 199/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1473 - accuracy: 0.9496\n",
      "Epoch 200/200\n",
      "126/126 [==============================] - 1s 8ms/step - loss: 0.1431 - accuracy: 0.9491\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_to_model, label, epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2_J0wuQn4_T"
   },
   "source": [
    "Let's plot details of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "UcFNM7imn4_U",
    "outputId": "b135e707-8a97-48f9-da42-62cb49e82e1b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N28e9LOPn4_W"
   },
   "source": [
    "# Generating fake comments\n",
    "\n",
    "To generate fake tweets, we use the below architecture:\n",
    "\n",
    "![](imgs/text_gen.png)\n",
    "\n",
    "The idea is to give one or more starting token(s) to our model, and generate the next tokens until we generate `.`.\n",
    "\n",
    "At each step, we select the token with the highest probability as our next token and generate the next one similartly using `model.predict_classes()`. \n",
    "\n",
    "**Note:** The model takes as input the activation `a` from the previous state of the LSTM and the token chosen, forward propagate by one step, and get a new output activation `a`. The new activation `a` can then be used to generate the output, using the `dense` layer with `softmax` activation as before. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oktWaXcn4_W"
   },
   "source": [
    "**Task 2**: Implement `generate()`. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBQKtBhin4_X"
   },
   "outputs": [],
   "source": [
    "#TASK 2\n",
    "# Implement the generate() function\n",
    "\n",
    "def generate(seed_text):\n",
    "    ### START CODE HERE ### \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    sentence = seed_text\n",
    "    a = ''\n",
    "    while a != '.':\n",
    "      \n",
    "        corpus = sentence.lower().split(\"\\n\")\n",
    "\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "        input_sequences = []\n",
    "        for l in corpus:\n",
    "            token_list = tokenizer.texts_to_sequences([l])[0]\n",
    "                #for i in range(1, len(token_list)):\n",
    "                   # n_gram_sequence = token_list[:i+1]\n",
    "                   # print(n_gram_sequence)\n",
    "            input_sequences.append(token_list)\n",
    "        \n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len-1, padding='pre'))\n",
    "        prediction = model.predict_classes(input_sequences, verbose=0)\n",
    "        if prediction[0] != 0:\n",
    "          w = reverse_word_map[prediction[0]]\n",
    "        sentence += ' '+ w\n",
    "        a = w\n",
    "    return sentence\n",
    "    ### END CODE HERE ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-Wd0L_zn4_Z"
   },
   "source": [
    "**Let's test it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "colab_type": "code",
    "id": "JIL3TYRPn4_Z",
    "outputId": "d477255a-e1c8-4a38-90fc-45afdd4823bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "COVID19 virus covid19 of virus covid19 because a kill covid19 test it for your on the the the government the military of the .\n",
      "\n",
      "\n",
      "\n",
      "COVID19 is the outbreak senegal be 7 test for on on at the home the the the all like get getting of to in the , can so the and should so the all and is is .\n",
      "\n",
      "\n",
      "\n",
      "The usa is outbreak senegal be 7 test for on on at the home the home the , coronavirus states covid19 of the , two are the , up can so the and should cvd that them door at down citizens up , .\n",
      "\n",
      "\n",
      "\n",
      "The new virus outbreak senegal be 7 test for on on at the home the home the , coronavirus states covid19 of the , two are the , up can so the and should cvd that them door at down citizens up , .\n",
      "\n",
      "\n",
      "\n",
      "China has covid19 outbreak senegal be 7 test for on on at the the the the on disease coronavirus kill the of and to have the are can , the and to in kills a that not door for it very have the coronavirus means city .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print(generate(\"COVID19 virus\"))\n",
    "print('\\n\\n')\n",
    "print(generate(\"COVID19 is the\"))\n",
    "print('\\n\\n')\n",
    "print(generate(\"The usa is\"))\n",
    "print('\\n\\n')\n",
    "print(generate(\"The new virus\"))\n",
    "print('\\n\\n')\n",
    "print(generate(\"China has\"))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VW2b9IFXn4_b"
   },
   "source": [
    "**Let's test it in an interactive mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Bwk5nsIon4_c",
    "outputId": "7b77d2af-e2ab-4632-8196-60fec04b6233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your tweet, the algorithm machine will complete it. Your input is: covid19 was\n",
      "covid19 was covid19 of virus a be covid19 test a kill for . "
     ]
    }
   ],
   "source": [
    "usr_input = input(\"Write the beginning of your tweet, the algorithm machine will complete it. Your input is: \")\n",
    "for w in generate(usr_input).split():    \n",
    "    print(w, end =\" \")\n",
    "    time.sleep(0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nk3xpTyIn4_e"
   },
   "source": [
    "# Generating text by sampling\n",
    "\n",
    "The previous part is generating text by choosing the token with the highest probability. Now, we sill generate text by sampling as shown in the architecture below:\n",
    "\n",
    "![](imgs/text_gen_sample.png)\n",
    "\n",
    "\n",
    "**TASK 3:** Implement the `generate_sample()` function. To sample a token from the output at each timestep, you need to use the following two functions:\n",
    "- `model.predict_proba()`: To get probabilities from the output layer.\n",
    "- `np.random.choice()`: To sample from the token list using the probaility array of each token.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nos5GwA7n4_e"
   },
   "outputs": [],
   "source": [
    "#TASK 3\n",
    "# Implement the generate_sample() function\n",
    "def generate_sample(seed_text):\n",
    "    \n",
    "   ### START CODE HERE ### \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    sentence = seed_text\n",
    "    a = ''\n",
    "    count = 0\n",
    "    while a != '.':\n",
    "      \n",
    "        corpus = sentence.lower().split(\"\\n\")\n",
    "\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "        input_sequences = []\n",
    "        for l in corpus:\n",
    "            token_list = tokenizer.texts_to_sequences([l])[0]\n",
    "            input_sequences.append(token_list)\n",
    "        \n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len-1, padding='pre'))\n",
    "        prediction = model.predict_proba(input_sequences, verbose=0)\n",
    "        p = np.random.choice(prediction[0])\n",
    "        p = np.where(prediction[0] == p)\n",
    "        if p[0][0] != 0:\n",
    "          w = reverse_word_map[p[0][0]]\n",
    "        sentence += ' '+ w\n",
    "        a = w\n",
    "        count+=1\n",
    "        if(count == 50):\n",
    "          break\n",
    "\n",
    "    return sentence\n",
    "    ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSalPA6on4_g"
   },
   "source": [
    "**Let's test it in an interactive mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "GESrNBckn4_g",
    "outputId": "0f760865-bf87-4c05-e441-ce63970bb892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your tweet, the algorithm machine will complete it. Your input is: kids cant\n",
      "\n",
      "\n",
      "kids cant chronic tweeted food prophesied across go own medical against center that exposed clorox reduces february covid19 lotteries agendas your never whitmer risk lemon sunlight terrorism kill released economy see hell disease gunpoint use banner weapon already spread bought knocks agreed you suggests whole mobile these governors vaping cattle italy chance "
     ]
    }
   ],
   "source": [
    "usr_input = input(\"Write the beginning of your tweet, the algorithm machine will complete it. Your input is: \")\n",
    "print('\\n') \n",
    "for w in generate_sample(usr_input).split():   \n",
    "    print(w, end =\" \")\n",
    "    time.sleep(0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzCfm-6In4_i"
   },
   "source": [
    "# Generate your own text \n",
    "\n",
    "Below, use you own data to generate content for a different application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "LRVCOm-An5AR",
    "outputId": "cdc2e0a4-3650-4163-8adb-98123c33b363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of fake tweets:  ['to sherlock holmes she is always _the_ woman  .', '  i have seldom heard him\\nmention her under any other name  .']\n",
      "Size of the vocabulary =  324\n",
      "Example of our word index =  [(',', 1), ('the', 2), ('and', 3), ('of', 4), ('his', 5), ('to', 6), ('was', 7), ('a', 8), ('in', 9), ('i', 10)]\n",
      "(636,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "data = open('Sherlock.txt').read().replace(\".\", \" . \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"!\", \" ! \")\n",
    "\n",
    "corpus = data.lower().split(\".\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "c=[]\n",
    "for l in corpus:\n",
    "    c.append(l + ' .')\n",
    "\n",
    "print(\"Example of fake tweets: \",c[:2])\n",
    "print(\"Size of the vocabulary = \", total_words)\n",
    "index = [(k, v) for k, v in tokenizer.word_index.items()]\n",
    "print(\"Example of our word index = \", index[0:10])\n",
    "\n",
    "input_sequences = []\n",
    "for line in c:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "input_to_model, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "print(label.shape)\n",
    "\n",
    "label = ku.to_categorical(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "vHbygoUBXAfa",
    "outputId": "78a02435-8f54-4dcd-c0c7-93962c6ed272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(636, 88)\n",
      "(636, 324)\n",
      "(636, 89)\n"
     ]
    }
   ],
   "source": [
    "print(input_to_model.shape)\n",
    "print(label.shape)\n",
    "print(input_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ft91Vrp4XAfc"
   },
   "outputs": [],
   "source": [
    "size = 128\n",
    "def deep_fake_comment_model1():\n",
    "    ### START CODE HERE ### \n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(324, size, input_length=88))\n",
    "    model1.add(LSTM(size))\n",
    "    model1.add(Dropout(0.1))\n",
    "    model1.add(Dense(324, activation='softmax'))\n",
    "    model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model1\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "#Print details of the model.\n",
    "model1 = deep_fake_comment_model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LlOKlgAZXAfe",
    "outputId": "e8ee940d-2af8-4f90-fe19-8b7db8962edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - 0s 11ms/step - loss: 5.6789 - accuracy: 0.0487\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 5.2698 - accuracy: 0.0692\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 5.1677 - accuracy: 0.0519\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.1255 - accuracy: 0.0692\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.1049 - accuracy: 0.0676\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.0675 - accuracy: 0.0708\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 5.0303 - accuracy: 0.0802\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.9644 - accuracy: 0.0723\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 4.8665 - accuracy: 0.0912\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.7560 - accuracy: 0.1179\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.6268 - accuracy: 0.1368\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.4869 - accuracy: 0.1572\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 4.3209 - accuracy: 0.1698\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 4.1597 - accuracy: 0.1871\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 4.0063 - accuracy: 0.1997\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.8251 - accuracy: 0.2280\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.6740 - accuracy: 0.2437\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.5219 - accuracy: 0.2594\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.3613 - accuracy: 0.2783\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 3.2146 - accuracy: 0.3050\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 3.0589 - accuracy: 0.3223\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.9148 - accuracy: 0.3365\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 2.7734 - accuracy: 0.3711\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.6390 - accuracy: 0.4104\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.5063 - accuracy: 0.4450\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.3719 - accuracy: 0.4796\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 2.2295 - accuracy: 0.5126\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 2.0907 - accuracy: 0.5833\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.9966 - accuracy: 0.6242\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.8809 - accuracy: 0.6604\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.7531 - accuracy: 0.7044\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.6589 - accuracy: 0.7531\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.5469 - accuracy: 0.7767\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.4584 - accuracy: 0.8255\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.3693 - accuracy: 0.8270\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.2624 - accuracy: 0.8711\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.1947 - accuracy: 0.8962\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.1183 - accuracy: 0.8884\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.0564 - accuracy: 0.9057\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.9793 - accuracy: 0.9245\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.9300 - accuracy: 0.9403\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8731 - accuracy: 0.9450\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.8206 - accuracy: 0.9528\n",
      "Epoch 44/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.7762 - accuracy: 0.9575\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7397 - accuracy: 0.9607\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.6828 - accuracy: 0.9670\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.6475 - accuracy: 0.9623\n",
      "Epoch 48/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.6098 - accuracy: 0.9733\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.5775 - accuracy: 0.9733\n",
      "Epoch 50/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.5472 - accuracy: 0.9764\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.5152 - accuracy: 0.9843\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.4858 - accuracy: 0.9717\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.4674 - accuracy: 0.9796\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.4377 - accuracy: 0.9843\n",
      "Epoch 55/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.4127 - accuracy: 0.9811\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3988 - accuracy: 0.9827\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3811 - accuracy: 0.9843\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3660 - accuracy: 0.9858\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3443 - accuracy: 0.9843\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3240 - accuracy: 0.9811\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3111 - accuracy: 0.9811\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2974 - accuracy: 0.9874\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.2880 - accuracy: 0.9811\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2740 - accuracy: 0.9874\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2717 - accuracy: 0.9811\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2498 - accuracy: 0.9858\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.2472 - accuracy: 0.9843\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.2373 - accuracy: 0.9827\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.2284 - accuracy: 0.9827\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.2208 - accuracy: 0.9811\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.2109 - accuracy: 0.9811\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.2081 - accuracy: 0.9843\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1986 - accuracy: 0.9827\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1959 - accuracy: 0.9843\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1823 - accuracy: 0.9921\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9796\n",
      "Epoch 77/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1784 - accuracy: 0.9843\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1748 - accuracy: 0.9858\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1669 - accuracy: 0.9827\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1649 - accuracy: 0.9796\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9890\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1488 - accuracy: 0.9874\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1443 - accuracy: 0.9827\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1476 - accuracy: 0.9811\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1398 - accuracy: 0.9827\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1404 - accuracy: 0.9796\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1280 - accuracy: 0.9843\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9811\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1346 - accuracy: 0.9827\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1275 - accuracy: 0.9827\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1228 - accuracy: 0.9811\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1240 - accuracy: 0.9858\n",
      "Epoch 93/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1145 - accuracy: 0.9811\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1137 - accuracy: 0.9796\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1078 - accuracy: 0.9827\n",
      "Epoch 96/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1074 - accuracy: 0.9827\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.9874\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.1031 - accuracy: 0.9874\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1037 - accuracy: 0.9843\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0983 - accuracy: 0.9827\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0972 - accuracy: 0.9858\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0964 - accuracy: 0.9827\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0962 - accuracy: 0.9858\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0949 - accuracy: 0.9843\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0928 - accuracy: 0.9843\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0962 - accuracy: 0.9843\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0901 - accuracy: 0.9874\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0892 - accuracy: 0.9843\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0878 - accuracy: 0.9858\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0851 - accuracy: 0.9843\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0839 - accuracy: 0.9858\n",
      "Epoch 112/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0853 - accuracy: 0.9827\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0814 - accuracy: 0.9843\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0789 - accuracy: 0.9874\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9858\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0786 - accuracy: 0.9827\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0784 - accuracy: 0.9843\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0767 - accuracy: 0.9827\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.9811\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9858\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9827\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0728 - accuracy: 0.9827\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0755 - accuracy: 0.9796\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0675 - accuracy: 0.9843\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0700 - accuracy: 0.9827\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0658 - accuracy: 0.9874\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0676 - accuracy: 0.9843\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0657 - accuracy: 0.9858\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0698 - accuracy: 0.9827\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0635 - accuracy: 0.9858\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0676 - accuracy: 0.9827\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0680 - accuracy: 0.9843\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0625 - accuracy: 0.9843\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.0618 - accuracy: 0.9843\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0644 - accuracy: 0.9843\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 0.9874\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0621 - accuracy: 0.9827\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9827\n",
      "Epoch 139/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0603 - accuracy: 0.9827\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0591 - accuracy: 0.9843\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0548 - accuracy: 0.9890\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0559 - accuracy: 0.9858\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0574 - accuracy: 0.9843\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0594 - accuracy: 0.9827\n",
      "Epoch 145/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0574 - accuracy: 0.9858\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0588 - accuracy: 0.9858\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0617 - accuracy: 0.9827\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0617 - accuracy: 0.9827\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0553 - accuracy: 0.9843\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0516 - accuracy: 0.9843\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0536 - accuracy: 0.9843\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.9811\n",
      "Epoch 153/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0554 - accuracy: 0.9843\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0562 - accuracy: 0.9843\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0540 - accuracy: 0.9843\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0547 - accuracy: 0.9843\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0520 - accuracy: 0.9811\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0517 - accuracy: 0.9811\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0504 - accuracy: 0.9843\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0545 - accuracy: 0.9843\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0509 - accuracy: 0.9858\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0511 - accuracy: 0.9827\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0526 - accuracy: 0.9811\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9843\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0528 - accuracy: 0.9811\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0516 - accuracy: 0.9827\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0476 - accuracy: 0.9858\n",
      "Epoch 168/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9827\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0475 - accuracy: 0.9858\n",
      "Epoch 170/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0476 - accuracy: 0.9827\n",
      "Epoch 171/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0483 - accuracy: 0.9843\n",
      "Epoch 172/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0483 - accuracy: 0.9843\n",
      "Epoch 173/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0471 - accuracy: 0.9811\n",
      "Epoch 174/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9843\n",
      "Epoch 175/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0445 - accuracy: 0.9843\n",
      "Epoch 176/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0401 - accuracy: 0.9890\n",
      "Epoch 177/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0454 - accuracy: 0.9874\n",
      "Epoch 178/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0445 - accuracy: 0.9874\n",
      "Epoch 179/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0457 - accuracy: 0.9843\n",
      "Epoch 180/200\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.0435 - accuracy: 0.9811\n",
      "Epoch 181/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0481 - accuracy: 0.9843\n",
      "Epoch 182/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9858\n",
      "Epoch 183/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 184/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0414 - accuracy: 0.9858\n",
      "Epoch 185/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0420 - accuracy: 0.9858\n",
      "Epoch 186/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.9874\n",
      "Epoch 187/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0422 - accuracy: 0.9858\n",
      "Epoch 188/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0412 - accuracy: 0.9843\n",
      "Epoch 189/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0455 - accuracy: 0.9811\n",
      "Epoch 190/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0456 - accuracy: 0.9811\n",
      "Epoch 191/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 0.9874\n",
      "Epoch 192/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0423 - accuracy: 0.9811\n",
      "Epoch 193/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0389 - accuracy: 0.9858\n",
      "Epoch 194/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0449 - accuracy: 0.9827\n",
      "Epoch 195/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0434 - accuracy: 0.9858\n",
      "Epoch 196/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9843\n",
      "Epoch 197/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 0.9874\n",
      "Epoch 198/200\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.0392 - accuracy: 0.9874\n",
      "Epoch 199/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 0.9890\n",
      "Epoch 200/200\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(input_to_model, label, epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "3Zzav3MZXAff",
    "outputId": "46640e0b-705e-4939-9874-3518c24d79b7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfEJbJpRXAfh"
   },
   "outputs": [],
   "source": [
    "def generate_sample1(seed_text):\n",
    "    ### START CODE HERE ### \n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    sentence = seed_text\n",
    "    a = ''\n",
    "    while a != '.':\n",
    "      \n",
    "        corpus = sentence.lower().split(\"\\n\")\n",
    "\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "        input_sequences = []\n",
    "        for l in corpus:\n",
    "            token_list = tokenizer.texts_to_sequences([l])[0]\n",
    "            input_sequences.append(token_list)\n",
    "        input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len-1, padding='pre'))\n",
    "        prediction = model1.predict_classes(input_sequences, verbose=0)\n",
    "        if prediction[0] != 0:\n",
    "          w = reverse_word_map[prediction[0]]\n",
    "        sentence += ' ' + w\n",
    "        a = w\n",
    "    return sentence\n",
    "    ### END CODE HERE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "zwrRM-KsXAfk",
    "outputId": "52154e18-b9e5-45b3-a1ae-b5307e3e251e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your tweet, the algorithm machine will complete it. Your input is: My marriage had drifted\n",
      "\n",
      "\n",
      "My marriage had drifted the flu has flu have her invented years that can it home that reports italy spread it it , , , it the days it days from viruses folks the the the the the the folks which out lot should should a . "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use - 'My marriage had drifted'))\n",
    "\"\"\"\n",
    "usr_input = input(\"Write the beginning of your tweet, the algorithm machine will complete it. Your input is: \")\n",
    "print('\\n')\n",
    "for w in generate_sample1(usr_input).split():    \n",
    "    print(w, end =\" \")\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqHwr58Bn5AX"
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You've come to the end of this assignment, and have seen how to build a deep learning architecture that generate fake tweets/comments. \n",
    "\n",
    "Congratulations on finishing this notebook! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
